# src/models/evaluate_model.py

import os
import json
import logging
import matplotlib.pyplot as plt
import seaborn as sns

from typing import Optional, Union
import numpy as np
import pandas as pd

from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    roc_auc_score,
    precision_recall_curve,
    average_precision_score,
    accuracy_score,
    recall_score,
    precision_score,
    f1_score,
    balanced_accuracy_score
)

from models.utils import load_model  

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Fonction principale


def evaluate_classifier(
    model_path: str,
    X_test: Union[np.ndarray, pd.DataFrame],
    y_test: Union[np.ndarray, pd.Series],
    output_dir: str = "reports",
    model_name: str = "logreg_toxic",
    display: bool = False,
    return_scores: bool = False
) -> Optional[dict]:
    """
    √âvalue un mod√®le binaire :
    G√©n√®re :
    - classification_report.json
    - confusion_matrix.png
    - ROC.png
    - Precision-Recall.png
    - summary.md

    :param model_path: chemin vers le mod√®le .pkl
    :param X_test: matrice de features test
    :param y_test: vecteur des vraies classes
    :param output_dir: dossier de sortie
    :param model_name: nom du mod√®le pour le pr√©fixe
    :param display: si True, affiche les graphes √† l'√©cran
    :param return_scores: si True, retourne le dict des m√©triques
    """

    # S√©curit√©
    os.makedirs(output_dir, exist_ok=True)
    report_prefix = os.path.join(output_dir, model_name)

    # Chargement mod√®le
    logger.info(f"üì• Chargement du mod√®le depuis : {model_path}")
    model = load_model(model_path)

    # Pr√©dictions
    logger.info("üîÆ Pr√©diction...")
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # M√©triques globales
    
    logger.info("Calcul des scores")
    scores = {
        "accuracy": accuracy_score(y_test, y_pred),
        "balanced_accuracy": balanced_accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1_score": f1_score(y_test, y_pred),
        "roc_auc": roc_auc_score(y_test, y_proba),
        "average_precision": average_precision_score(y_test, y_proba)
    }

    # Rapport d√©taill√© JSON
    report_path = f"{report_prefix}_report.json"
    classif_report = classification_report(y_test, y_pred, output_dict=True)
    with open(report_path, "w") as f:
        json.dump(classif_report, f, indent=2)
    logger.info(f"Rapport de classification sauvegard√© ‚Üí {report_path}")

    # Matrice de confusion
    
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Non toxique", "Toxique"],
                yticklabels=["Non toxique", "Toxique"])
    plt.title("Matrice de confusion")
    plt.xlabel("Pr√©diction")
    plt.ylabel("V√©rit√© terrain")
    plt.tight_layout()
    cm_path = f"{report_prefix}_confusion_matrix.png"
    plt.savefig(cm_path)
    if display: plt.show()
    plt.close()
    logger.info(f"Matrice de confusion sauvegard√©e ‚Üí {cm_path}")

    #  ROC Curve
    
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    auc = scores["roc_auc"]
    plt.figure()
    plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}")
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.xlabel("Taux de faux positifs")
    plt.ylabel("Taux de vrais positifs")
    plt.title("Courbe ROC")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    roc_path = f"{report_prefix}_roc_curve.png"
    plt.savefig(roc_path)
    if display: plt.show()
    plt.close()
    logger.info(f"Courbe ROC sauvegard√©e ‚Üí {roc_path}")

    # Courbe Pr√©cision-Rappel
    
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    ap = scores["average_precision"]
    plt.figure()
    plt.plot(recall, precision, label=f"AP = {ap:.2f}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Courbe Pr√©cision-Recall")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    pr_path = f"{report_prefix}_precision_recall_curve.png"
    plt.savefig(pr_path)
    if display: plt.show()
    plt.close()
    logger.info(f"Courbe Pr√©cision-Recall sauvegard√©e ‚Üí {pr_path}")

    # Rapport Markdown (summary)
    
    md_path = f"{report_prefix}_summary.md"
    with open(md_path, "w") as md:
        md.write(f"# √âvaluation du mod√®le `{model_name}`\n\n")
        md.write("## Scores globaux\n")
        for metric, val in scores.items():
            md.write(f"- **{metric}** : `{val:.4f}`\n")
        md.write("\n## Matrice de confusion\n")
        md.write(f"![]({os.path.basename(cm_path)})\n")
        md.write("\n## ROC Curve\n")
        md.write(f"![]({os.path.basename(roc_path)})\n")
        md.write("\n## Precision-Recall\n")
        md.write(f"![]({os.path.basename(pr_path)})\n")
        md.write("\n---\n_G√©n√©r√© automatiquement avec `evaluate_classifier()`_\n")

    logger.info(f"Rapport Markdown g√©n√©r√© ‚Üí {md_path}")

    if return_scores:
        return scores
